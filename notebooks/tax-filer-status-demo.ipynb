{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Import workspace and other libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# Filepath\n",
    "script_folder = os.getcwd()\n",
    "filepath = script_folder + '/census-income-data.csv'\n",
    "print(filepath + '\\n')\n",
    "\n",
    "# Convert csv to datafreame\n",
    "df = pd.read_csv(filepath)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(\"converted dataset to dataframe\\n\")\n",
    "\n",
    "## Start preprocessing: Clean up data and normalize\n",
    "\n",
    "# Replace missing data\n",
    "df_nan = df.replace('?', np.nan)\n",
    "print(\"missing data replaced with NAN\\n\")\n",
    "\n",
    "# trim all columns\n",
    "df_clean = df_nan.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "print(\"trimmed all leading and trailing spaces\\n\")\n",
    "\n",
    "print(\"preprocessing complete\\n\")\n",
    "\n",
    "# Separate labels from measures\n",
    "y_raw = df_clean.pop('Tax_Filer_Status')\n",
    "\n",
    "X = df_clean\n",
    "print(\"label separation complete\\n\")\n",
    "\n",
    "# Drop columns with too many missing values (all migration-related columns)\n",
    "X.drop(X.columns[[20]], axis = 1, inplace = True)\n",
    "print(\"dropped instance weight\\n\")\n",
    "\n",
    "# Encode features\n",
    "x_1hot = pd.get_dummies(X, drop_first=True)\n",
    "print(\"featrure encoding complete\\n\")\n",
    "\n",
    "# Encode labels\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "Y = label_enc.fit_transform(y_raw)\n",
    "print(\"label encoding complete\\n\")\n",
    "\n",
    "feature_names = np.array(x_1hot.columns.values)\n",
    "print(\"Defined feature names and classes\\n\")\n",
    "\n",
    "labels = np.array(['tax filer', 'non-filer']).tolist()\n",
    "print(\"classes have been defined\\n\")\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x_1hot, Y, test_size=0.2)\n",
    "print(\"split dataset into training and test sets with an 80-20 partition\\n\")\n",
    "\n",
    "# Flatten label column to fit the SVM classifier requirement (otherwise you get warnings)\n",
    "y_train_flat = y_train.ravel()\n",
    "y_test_flat = y_test.ravel()\n",
    "print(\"flattened class label array\\n\")\n",
    "\n",
    "# Impute missing values using univariate imputer (multivariate is not supported in this version of Scikit-learn 0.18.1)\n",
    "print(\"imputing missing values as nan\\n\")\n",
    "imp = Imputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp.fit(X_train)\n",
    "X_train_clean = imp.transform(X_train)\n",
    "X_test_clean = imp.transform(X_test)\n",
    "\n",
    "print(\"imputation complete\\n\")\n",
    "\n",
    "\n",
    "## Normalize dataset (exclude labels)\n",
    "#X_norm = normalize(X_clean)\n",
    "\n",
    "## Standardize dataset\n",
    "#scaler = StandardScaler()  \n",
    "\n",
    "## Fit only on training data\n",
    "#scaler.fit(X_train_clean)  \n",
    "#x_train = scaler.transform(X_train_clean)\n",
    "\n",
    "## apply same transformation to test data\n",
    "#x_test = scaler.transform(X_test_clean)  \n",
    "\n",
    "#print(\"standardized dataset\\n\")\n",
    "\n",
    "\n",
    "## Train a neural network\n",
    "clf = MLPClassifier(solver='sgd',\n",
    "                    hidden_layer_sizes=(15,30,15), random_state=1)\n",
    "\n",
    "model = clf.fit(X_train_clean, y_train_flat)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test_clean)\n",
    "print(confusion_matrix(y_test_flat,predictions))\n",
    "print()\n",
    "print(classification_report(y_test_flat,predictions))\n",
    "\n",
    "## De-standardize data\n",
    "# x_train_dstd = scaler.inverse_transform(x_train)\n",
    "# x_test_dstd = scaler.inverse_transform(x_test)\n",
    "\n",
    "\n",
    "## Start explanation\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "explainer = TabularExplainer(model, \n",
    "                            X_test_clean, # x_test\n",
    "                            features=feature_names, \n",
    "                            classes=labels)\n",
    "print(\"initialized explainer\\n\")\n",
    "\n",
    "# Global explanation\n",
    "# you can use the training data or the test data here\n",
    "global_explanation = explainer.explain_global(X_test_clean[0:100])\n",
    "\n",
    "# if you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# global_explanation = explainer.explain_global(x_train, true_labels=y_test)\n",
    "\n",
    "# sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "dict(zip(sorted_global_importance_names, sorted_global_importance_values))\n",
    "\n",
    "# alternatively, you can print out a dictionary that holds the top K feature names and values\n",
    "global_explanation.get_feature_importance_dict()\n",
    "\n",
    "\n",
    "#Local explanation\n",
    "# explain the first data point in the test set\n",
    "local_explanation = explainer.explain_local(X_test_clean[0:100])\n",
    "\n",
    "# sorted feature importance values and feature names\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "\n",
    "\n",
    "# Display visualization dashboard\n",
    "from azureml.contrib.interpret.visualize import ExplanationDashboard\n",
    "\n",
    "ExplanationDashboard(global_explanation, model, X_test_clean[0:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
